{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complex-river",
   "metadata": {},
   "source": [
    "# You Do Session 2\n",
    "- Importing Libraries and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crazy-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attended-cowboy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  4., ..., nan, nan, nan],\n",
       "       [ 4., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [ 5., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan,  5., nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uploading data and making it rating matrix \n",
    "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',\n",
    "names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "R = df.pivot(index='user_id', columns='item_id', values='rating').values\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-adobe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for the shape of the matrix\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collectible-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need nan values to fill the matrix, so we need to mask the data \n",
    "# find indexes of non-zero elements\n",
    "irow, jcol = np.where(~np.isnan(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-keeping",
   "metadata": {},
   "source": [
    "## 1 ) Non-regularized version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-lounge",
   "metadata": {},
   "source": [
    "### Let's create our model, our loss functions\n",
    "\n",
    "- Our model will be bias based as declared in the homework description. \n",
    "\n",
    "- $$ \\hat{r_{ij}} = b_i^{user} + b_j^{item} $$\n",
    "\n",
    "- As we declared above, we should optimize our b values over non-zero values. Let's call this subset S and define it as follows:\n",
    "\n",
    "- $$  S = \\left \\{ (i,j) : r_{ij}\\:is\\: observed \\right \\} $$ \n",
    "\n",
    "- Then our loss function will look like this \n",
    "\n",
    "- $$ loss = \\frac{1}{2} * \\sum_{(i,j) \\in S }( r_{ij} - \\hat{r_{ij}})^{2}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fiscal-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test sets\n",
    "def split_data(R,irow,jcol):\n",
    "    idx = np.random.choice(np.arange(100_000), 10000, replace=False)\n",
    "    test_irow = irow[idx]\n",
    "    test_jcol = jcol[idx]\n",
    "    R_copy = R.copy()\n",
    "    R_copy[test_irow, test_jcol] = np.nan\n",
    "    return R_copy, test_irow, test_jcol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "social-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def loss(b_user, b_item, R, irow, jcol):\n",
    "    loss = 0\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        loss += (R[i, j] - b_user[i] - b_item[j]) ** 2 * 0.5\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "right-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gradient of loss function\n",
    "def gradient(b_user, b_item, R, irow, jcol):\n",
    "    b_user_grad = np.zeros(b_user.shape)\n",
    "    b_item_grad = np.zeros(b_item.shape)\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        b_user_grad[i] += (R[i, j] - b_user[i] - b_item[j])\n",
    "        b_item_grad[j] += (R[i, j] - b_user[i] - b_item[j])\n",
    "\n",
    "    return b_user_grad, b_item_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "handmade-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gradient descent function\n",
    "def gradient_descent(R, b_user, b_item, irow, jcol, lr, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        b_user_grad, b_item_grad = gradient(b_user, b_item, R, irow, jcol)\n",
    "        prev_b_user = b_user.copy()\n",
    "        prev_b_item = b_item.copy()\n",
    "        b_user += lr * b_user_grad\n",
    "        b_item += lr * b_item_grad\n",
    "        if epoch % 10 == 0:\n",
    "            print('loss:', loss(b_user, b_item, R, irow, jcol))\n",
    "        # very early stopping\n",
    "        if np.linalg.norm(b_user - prev_b_user) < 1e-2 and np.linalg.norm(b_item - prev_b_item) < 1e-2:\n",
    "            break\n",
    "\n",
    "    return b_user, b_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "together-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE\n",
    "def rmse(R, b_user, b_item, irow, jcol):\n",
    "    return np.sqrt(2 * loss(b_user, b_item, R, irow, jcol) / len(irow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sharp-internship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 512701.9190788698\n",
      "loss: 113572.47119923071\n",
      "loss: 76150.66835639812\n",
      "loss: 62733.18443845988\n",
      "loss: 55675.78633463886\n",
      "loss: 51327.840741421125\n",
      "loss: 48404.009012990886\n",
      "loss: 46324.54118858998\n",
      "loss: 44785.94704069288\n",
      "loss: 43613.0230151942\n",
      "loss: 42697.4802643668\n",
      "loss: 41968.922713540596\n",
      "loss: 41379.7512497769\n",
      "loss: 40896.73655421998\n",
      "loss: 40496.0532920679\n",
      "loss: 40160.2256277335\n",
      "loss: 39876.180843786344\n",
      "loss: 39633.97130561652\n",
      "loss: 39425.91311176596\n",
      "loss: 39245.9920486845\n",
      "loss: 39089.44538291593\n",
      "loss: 38952.461949876146\n",
      "loss: 38831.96346042623\n",
      "loss: 38725.44260807699\n",
      "loss: 38630.841576217856\n",
      "loss: 38546.459727342946\n",
      "loss: 38470.882671411746\n",
      "loss: 38402.92720118798\n",
      "loss: 38341.59814444658\n",
      "loss: 38286.05426447703\n",
      "loss: 38235.58110003209\n",
      "loss: 38189.56917662066\n",
      "loss: 38147.49641083617\n",
      "loss: 38108.913813627136\n",
      "loss: 38073.43380788024\n",
      "loss: 38040.72063176197\n",
      "loss: 38010.48241645246\n",
      "loss: 37982.46461590825\n",
      "loss: 37956.44453420481\n",
      "loss: 37932.226748467685\n",
      "loss: 37909.63926601646\n",
      "loss: 37888.53028611377\n",
      "loss: 37868.76546169995\n",
      "loss: 37850.22557622866\n",
      "loss: 37832.80456642475\n",
      "loss: 37816.40783435738\n",
      "loss: 37800.95080229803\n",
      "loss: 37786.35767197751\n",
      "loss: 37772.56035645318\n",
      "loss: 37759.497558189134\n",
      "loss: 37747.113971318795\n",
      "loss: 37735.35958969132\n",
      "loss: 37724.18910523477\n",
      "loss: 37713.56138366173\n",
      "loss: 37703.439006543565\n",
      "loss: 37693.78787048031\n",
      "loss: 37684.57683549701\n",
      "loss: 37675.77741598682\n",
      "loss: 37667.36350849695\n",
      "loss: 37659.311151491056\n",
      "loss: 37651.59831291086\n",
      "loss: 37644.20470197966\n",
      "loss: 37637.111602148216\n",
      "loss: 37630.30172255728\n",
      "loss: 37623.75906570752\n",
      "loss: 37617.468809377584\n",
      "loss: 37611.41720106495\n",
      "loss: 37605.591463452576\n",
      "loss: 37599.9797096314\n",
      "loss: 37594.570866920534\n",
      "loss: 37589.35460832633\n",
      "loss: 37584.321290757376\n",
      "loss: 37579.46189925756\n",
      "loss: 37574.767996580806\n",
      "loss: 37570.23167754463\n",
      "loss: 37565.84552762799\n",
      "loss: 37561.60258538353\n",
      "loss: 37557.49630824983\n",
      "loss: 37553.520541424456\n",
      "loss: 37549.66948947431\n",
      "loss: 37545.93769041758\n",
      "loss: 37542.319992022436\n",
      "loss: 37538.81153011337\n",
      "loss: 37535.40770867833\n",
      "loss: 37532.10418162412\n",
      "loss: 37528.89683599728\n",
      "loss: 37525.78177655721\n",
      "loss: 37522.75531156354\n",
      "loss: 37519.81393967021\n",
      "loss: 37516.95433783111\n",
      "loss: 37514.17335011424\n",
      "loss: 37511.467977365945\n",
      "loss: 37508.83536762915\n",
      "loss: 37506.27280726192\n",
      "loss: 37503.7777127023\n",
      "loss: 37501.347622811016\n",
      "loss: 37498.9801917555\n",
      "loss: 37496.67318239076\n",
      "loss: 37494.424460087685\n",
      "loss: 37492.231986983745\n",
      "Non-Regularized RMSE for test set: 0.9467695715494107\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "R_copy, test_irow, test_jcol = split_data(R,irow,jcol)\n",
    "\n",
    "\n",
    "# initialize biases\n",
    "b_user = np.random.randn(943)\n",
    "b_item = np.random.randn(1682)\n",
    "\n",
    "\n",
    "# run gradient descent\n",
    "b_user, b_item = gradient_descent(R_copy, b_user, b_item, irow, jcol, 0.0005, 1000)\n",
    "\n",
    "# calculate RMSE on test set\n",
    "print('Non-Regularized RMSE for test set:' , rmse(R, b_user, b_item, test_irow, test_jcol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-infection",
   "metadata": {},
   "source": [
    "## 2 ) Regularized version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-amateur",
   "metadata": {},
   "source": [
    "### Let's create  our loss functions\n",
    "\n",
    "\n",
    "- Our loss function will look like this when we add regularized terms\n",
    "\n",
    " $$ regularized \\: loss = \\frac{1}{2} * \\sum_{(i,j) \\in S }( r_{ij} - \\hat{r_{ij}})^{2} + \\frac{\\lambda}{2} (\\sum_{i =1}^{m} (b_i^{user})^2 + \\sum_{j =1}^{n} (b_j^{item})^2  )   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "breathing-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized loss function\n",
    "def loss_reg(b_user, b_item, R, irow, jcol, lmbda):\n",
    "    loss = 0\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        loss += (R[i, j] - b_user[i] - b_item[j]) ** 2 * 0.5 + lmbda/2 * (b_user[i] ** 2 + b_item[j] ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "forty-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized gradient of loss function\n",
    "def gradient_reg(b_user, b_item, R, irow, jcol, lmbda):\n",
    "    b_user_grad = np.zeros(b_user.shape)\n",
    "    b_item_grad = np.zeros(b_item.shape)\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        b_user_grad[i] += (R[i, j] - b_user[i] - b_item[j]) - lmbda * b_user[i]\n",
    "        b_item_grad[j] += (R[i, j] - b_user[i] - b_item[j]) - lmbda * b_item[j]\n",
    "\n",
    "    return b_user_grad, b_item_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "loving-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized gradient descent function\n",
    "def gradient_descent_reg(R, b_user, b_item, irow, jcol, lmbda, lr, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        b_user_grad, b_item_grad = gradient_reg(b_user, b_item, R, irow, jcol, lmbda)\n",
    "        prev_b_user = b_user.copy()\n",
    "        prev_b_item = b_item.copy()\n",
    "        b_user += lr * b_user_grad\n",
    "        b_item += lr * b_item_grad\n",
    "        if epoch % 10 == 0:\n",
    "            print('loss:', loss_reg(b_user, b_item, R, irow, jcol, lmbda))\n",
    "        # very early stopping\n",
    "        if np.linalg.norm(b_user - prev_b_user) < 1e-2 and np.linalg.norm(b_item - prev_b_item) < 1e-2:\n",
    "            break\n",
    "\n",
    "    return b_user, b_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "transparent-truth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 524315.7959500112\n",
      "loss: 113947.7050960819\n",
      "loss: 76524.86772735107\n",
      "loss: 63077.09052528317\n",
      "loss: 55949.362315047234\n",
      "loss: 51533.62093178076\n",
      "loss: 48553.646871440695\n",
      "loss: 46429.943941172256\n",
      "loss: 44857.31565460672\n",
      "loss: 43658.593950065624\n",
      "loss: 42723.71141983058\n",
      "loss: 41980.78891420842\n",
      "loss: 41381.04163373995\n",
      "loss: 40890.316048047825\n",
      "loss: 40484.07780990949\n",
      "loss: 40144.31247894359\n",
      "loss: 39857.540473206005\n",
      "loss: 39613.507629853746\n",
      "loss: 39404.29926526799\n",
      "loss: 39223.727308991954\n",
      "loss: 39066.89789253693\n",
      "loss: 38929.900806005666\n",
      "loss: 38809.58287570565\n",
      "loss: 38703.38016141\n",
      "loss: 38609.19205025042\n",
      "loss: 38824.97985316854\n",
      "loss: 38750.10711924921\n",
      "loss: 38682.933007689964\n",
      "loss: 38622.37048189101\n",
      "loss: 38567.56972348508\n",
      "loss: 38517.81225840944\n",
      "loss: 38472.48633457268\n",
      "loss: 38431.06873092202\n",
      "loss: 38393.110148774176\n",
      "loss: 38358.22331006789\n",
      "loss: 38326.07319288693\n",
      "loss: 38296.36897811471\n",
      "loss: 38268.85737675034\n",
      "loss: 38243.31707778904\n",
      "loss: 38219.55411018914\n",
      "loss: 38197.397953964624\n",
      "loss: 38176.69826778673\n",
      "loss: 38157.32212604394\n",
      "loss: 38139.15167845587\n",
      "loss: 38122.082161442326\n",
      "loss: 38106.02020329123\n",
      "loss: 38090.88237554967\n",
      "loss: 38076.59395137601\n",
      "loss: 38063.08783840672\n",
      "loss: 38050.30365917207\n",
      "loss: 40750.677011409396\n",
      "loss: 40732.1785020541\n",
      "loss: 40721.3829403336\n",
      "loss: 40711.46985881842\n",
      "loss: 40702.10711200799\n",
      "loss: 40693.20590048331\n",
      "loss: 40684.71817641917\n",
      "loss: 40676.609640231\n",
      "loss: 40668.85258673287\n",
      "loss: 40661.423274552966\n",
      "loss: 40654.30072421015\n",
      "loss: 40647.466070977345\n",
      "loss: 40640.902169137735\n",
      "loss: 40634.59332491278\n",
      "loss: 40628.52510217363\n",
      "loss: 40622.68417279522\n",
      "loss: 40617.058196417216\n",
      "loss: 40611.635720706494\n",
      "loss: 40606.40609668807\n",
      "loss: 40601.35940560716\n",
      "loss: 40596.48639492335\n",
      "loss: 40591.77842176225\n",
      "loss: 40587.22740256664\n",
      "loss: 40582.82576801757\n",
      "loss: 40578.566422472584\n",
      "loss: 67150.5451792764\n",
      "loss: 66401.3774719202\n",
      "loss: 66324.00401391464\n",
      "loss: 66285.37685315822\n",
      "loss: 66258.15344313586\n",
      "loss: 66236.54554711732\n",
      "loss: 66218.36332576802\n",
      "loss: 66202.53566620461\n",
      "loss: 66188.45369674415\n",
      "loss: 66175.73531103689\n",
      "loss: 66164.1231462605\n",
      "loss: 66153.43426390094\n",
      "loss: 66143.53292877218\n",
      "loss: 66134.31484040296\n",
      "loss: 66125.69749997937\n",
      "loss: 66117.61406478951\n",
      "loss: 66110.00928040783\n",
      "loss: 66102.83669988273\n",
      "loss: 66096.0567272703\n",
      "loss: 66089.63520509475\n",
      "loss: 66083.54237040767\n",
      "loss: 66077.75206683575\n",
      "loss: 66072.2411383702\n",
      "loss: 66066.98895494243\n",
      "loss: 66061.97703529662\n",
      "loss: 277968.1175232287\n",
      "loss: 239196.0367836799\n",
      "loss: 236372.31487417602\n",
      "loss: 235342.44898804039\n",
      "loss: 234835.4161428409\n",
      "loss: 234551.35528271375\n",
      "loss: 234379.5588522903\n",
      "loss: 234270.02796912543\n",
      "loss: 234197.32472031965\n",
      "loss: 234147.42650395015\n",
      "loss: 234112.14857305348\n",
      "loss: 234086.50781125267\n",
      "loss: 234067.37113094758\n",
      "loss: 234052.71771236684\n",
      "loss: 234041.216301547\n",
      "loss: 234031.97371399638\n",
      "loss: 234024.3806937034\n",
      "loss: 234018.01516742905\n",
      "loss: 234012.58032706808\n",
      "loss: 234007.8643458827\n",
      "loss: 234003.71379403237\n",
      "loss: 234000.01587027474\n",
      "loss: 233996.68638711545\n",
      "loss: 233993.66155637323\n",
      "loss: 233990.8923135494\n"
     ]
    }
   ],
   "source": [
    "# cross-validation to find best lambda and save best model on test set\n",
    "b_user = np.random.randn(943)\n",
    "b_item = np.random.randn(1682)\n",
    "best_rmse = float('inf')\n",
    "best_lmbda = 0\n",
    "for lmbda in [0, 0.001, 0.01, 0.1, 1]:\n",
    "    b_user, b_item = gradient_descent_reg(R_copy, b_user, b_item, irow, jcol, lmbda, 0.0005, 250)\n",
    "    rmse_val = rmse(R, b_user, b_item, test_irow, test_jcol)\n",
    "    if rmse_val < best_rmse:\n",
    "        best_rmse = rmse_val\n",
    "        best_lmbda = lmbda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "relative-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 86245.40055533985\n",
      "loss: 46587.628422242946\n",
      "loss: 43551.26541806876\n",
      "loss: 42495.23640979734\n",
      "loss: 41931.92899519717\n",
      "loss: 41578.96255717097\n",
      "loss: 41338.00367333885\n",
      "loss: 41164.42544389834\n",
      "loss: 41034.617953019304\n",
      "loss: 40934.787368595986\n",
      "loss: 40856.3030272344\n",
      "loss: 40793.486611623906\n",
      "loss: 40742.45443285412\n",
      "loss: 40700.465963644994\n",
      "loss: 40665.536604673376\n",
      "loss: 40636.19734334324\n",
      "loss: 40611.34020903478\n",
      "loss: 40590.1158781658\n",
      "loss: 40571.864048363124\n",
      "loss: 40556.06499931369\n",
      "loss: 40542.30519956483\n",
      "loss: 40530.252436432325\n",
      "loss: 40519.63753476415\n",
      "loss: 40510.24071922339\n",
      "loss: 40501.881304613955\n",
      "loss: 40494.409808304845\n",
      "loss: 40487.70185031717\n",
      "loss: 40481.65338972584\n",
      "loss: 40476.17697174758\n",
      "loss: 40471.1987473216\n",
      "loss: 40466.65608892336\n",
      "loss: 40462.49567062005\n",
      "loss: 40458.67191251838\n",
      "loss: 40455.14571338737\n",
      "loss: 40451.88341270176\n",
      "loss: 40448.855936478525\n",
      "loss: 40446.03809123324\n",
      "loss: 40443.407977903\n",
      "Regularized RMSE: 0.9804548430366312\n"
     ]
    }
   ],
   "source": [
    "# run regularized gradient descent with best lambda\n",
    "b_user, b_item = gradient_descent_reg(R_copy, b_user, b_item, irow, jcol, best_lmbda, 0.0005, 1000)\n",
    "\n",
    "\n",
    "\n",
    "# function for regularized RMSE on test set with best model\n",
    "def rmse_reg(R, b_user, b_item, irow, jcol, lmbda):\n",
    "    return np.sqrt(2 * loss_reg(b_user, b_item, R, irow, jcol, lmbda) / len(irow))\n",
    "\n",
    "\n",
    "# calculate RMSE on test set\n",
    "print('Regularized RMSE:' , rmse_reg(R, b_user, b_item, test_irow, test_jcol, best_lmbda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-deadline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
